{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare FSE/VarBugs"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Set Baseline and Experiment\n",
    "tool = \"clang\" #clang, infer, phasar\n",
    "program = \"varbugs\" #axtls, toybox, busybox, varbugs\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "from json import JSONEncoder\n",
    "\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "\n",
    "from pathos import multiprocessing\n",
    "import logging\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import re\n",
    "from z3.z3 import Solver, And, Or, Not, Bool, Int, sat\n",
    "\n",
    "baselines: Path = Path(('preparedFSEBaselines/' if program != 'varbugs' else 'preparedVBDbBaselines/') + tool + program + \".json\")\n",
    "experimental_results: Path = Path('preparedExperimentResults/' + tool + program + '.json')\n",
    "\n",
    "with open(baselines) as f:\n",
    "    baselines = json.load(f)\n",
    "\n",
    "with open(experimental_results) as f:\n",
    "    experimental_results = json.load(f)\n",
    "\n",
    "lonely_baselines = copy.deepcopy(baselines)\n",
    "lonely_experimental_results = copy.deepcopy(experimental_results)\n",
    "\n",
    "class IntRange:\n",
    "    def __init__(self, lower_bound_inclusive, upper_bound_exclusive):\n",
    "        self.lower_bound_inclusive = lower_bound_inclusive\n",
    "        self.upper_bound_exclusive = upper_bound_exclusive\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return isinstance(item, int) and (self.lower_bound_inclusive <= item < self.upper_bound_exclusive)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"IntRange({self.lower_bound_inclusive, self.upper_bound_exclusive})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"[{self.lower_bound_inclusive}:{self.upper_bound_exclusive})\"\n",
    "\n",
    "    def to_json(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "for e in experimental_results:\n",
    "    toks = e['original_line'].split(':')\n",
    "    try:\n",
    "        e['original_line'] = IntRange(int(toks[0]), int(toks[1]) + 1)\n",
    "    except Exception as ex:\n",
    "        e['original_line'] = []\n",
    "    #print('\\t'.join([\"experimental\", *[str(s) for s in e.values()]]).replace(\"\\n\", \"\"))\n",
    "\n",
    "    if e['function_line_range'] == 'ERROR':\n",
    "        e['function_line_range'] = []\n",
    "    else:\n",
    "        toks = e['function_line_range'].split(':')\n",
    "        try:\n",
    "            e['function_line_range'] = IntRange(int(toks[1]), int(toks[2]) + 1)\n",
    "        except Exception as ex:\n",
    "            logging.exception(f\"e was {e}\")\n",
    "    e['presence_condition'] = str(e['presence_condition'])\n",
    "\n",
    "print(f\"We have {len(baselines)} baseline results.\")\n",
    "print(f\"We have {len(experimental_results)} experimental results.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def match_stats(baseline_result: dict, experimental_result: dict) -> Tuple:\n",
    "    \"\"\"\n",
    "    Returns a vector of different match information.\n",
    "    (a, b, c)\n",
    "    a = True iff baseline and experimental have the same line number, message, and file.\n",
    "    b = True iff baseline and experimental have the same message, file, and baseline is within experimental's function scope.\n",
    "    c = True iff baseline's configuration is compatible with experimental's presence condition.\n",
    "    \"\"\"\n",
    "\n",
    "    a = (baseline_result['sanitized_message'].lstrip().rstrip() == experimental_result['sanitized_message'].lstrip().rstrip() and \\\n",
    "         baseline_result['input_line'] in experimental_result['original_line'] and\\\n",
    "         baseline_result['input_file'].split('.')[0] == experimental_result['input_file'].split('.')[0])\n",
    "\n",
    "    b = (baseline_result['sanitized_message'].lstrip().rstrip() == experimental_result['sanitized_message'].lstrip().rstrip() and \\\n",
    "         baseline_result['input_line'] in experimental_result['function_line_range'] and\\\n",
    "         baseline_result['input_file'].split('.')[0] == experimental_result['input_file'].split('.')[0])\n",
    "\n",
    "    c = False\n",
    "\n",
    "    if experimental_result['feasible'] and 'Or(None' not in experimental_result['presence_condition'] and experimental_result['presence_condition'] not in ['Or(None)', 'None'] and (a or b):  # Don't bother doing this expensive step when the file and line number are different.\n",
    "        baseline_var_mapping = {}\n",
    "        for var in baseline_result['configuration']:\n",
    "            if type(var) is str:\n",
    "                if var.startswith('DEF'):\n",
    "                    baseline_var_mapping[re.sub(r\"^(DEF_.*)\", r\"\\1\", var)] = True\n",
    "                elif var.startswith('UNDEF'):\n",
    "                    baseline_var_mapping[re.sub(r\"^UN(DEF_.*)\", r\"\\1\", var)] = False\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Don't know how to handle variable {var}\")\n",
    "\n",
    "        s = Solver()\n",
    "        for var, val in baseline_var_mapping.items():\n",
    "            var = Bool(var)\n",
    "            if val:\n",
    "                s.add(var)\n",
    "            else:\n",
    "                s.add(Not(var))\n",
    "\n",
    "        for mat in re.findall(\"DEF_[a-zA-Z0-9_]+\", experimental_result['presence_condition']):\n",
    "            exec(f\"{mat} = Bool('{mat}')\")\n",
    "           \n",
    "        for mat in re.findall(\"USE_[a-zA-Z0-9_]+\", experimental_result['presence_condition']):\n",
    "            exec(f\"{mat} = Int('{mat}')\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                s.add(eval(experimental_result['presence_condition']))  # TODO Definitely need to do more transformation here.\n",
    "                break\n",
    "            except NameError as ne:\n",
    "                var = re.search(\"name '(.*)' is not defined\", str(ne))\n",
    "                exec(f\"{var.group(1)} = Int('{var.group(1)}')\")\n",
    "        c = s.check() == sat\n",
    "    return a, b, c\n",
    "\n",
    "def tupleize(func, args): return func(*args), tuple(args)\n",
    "\n",
    "summary = {}\n",
    "\n",
    "# Note that results depend on the order of keys in this dictionary, because once we find a match_stats for one level we do not keep searching for the next.\n",
    "#  E.g., for a given report, we will first look for results with which it has a (True, True, True) report. If it has one, we do not continue searching for\n",
    "#  matches for (False, True, True), (True, False, True), etc.\n",
    "result_hierarchy = {(True, True, True): 0, (False, True, True): 0, (True, False, True): 0, (True, True, False): 0, (False, True, False): 0, (False, False, True): 0, (True, False, False): 0, (False, False, False): 0}\n",
    "\n",
    "report = []\n",
    "for b in tqdm.tqdm(baselines):\n",
    "    # Results are (baseline, desugared, match tuple)\n",
    "    results = [(b, e, match_stats(b, e)) for e in experimental_results]\n",
    "    found = False\n",
    "    for r in result_hierarchy.keys():\n",
    "        for res in results:\n",
    "            if res[2] == r:\n",
    "                found = True\n",
    "                result_hierarchy[r] += 1\n",
    "                # -----\n",
    "                # Here is where you compile information about any specific reports you need. This block of code\n",
    "                # iterates through all baselines and finds the highest level of matching that is available.\n",
    "                # So, for example, if you wanted to collect all of the unmatched originals, you would uncomment out this line of code:\n",
    "                #\n",
    "                if (r != (True, True, True) and r != (False, True, True)):\n",
    "                    report.append(res[0])\n",
    "                break # DO NOT DELETE THE BREAK!\n",
    "        if found:\n",
    "            break\n",
    "print(len(report),'/',len(baselines))\n",
    "for r in report:\n",
    "    for key in ['id','input_file','input_line','message','warning_path']:\n",
    "        print(key,':',r[key])"
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of baseline results: {len(baselines)}\")\n",
    "print(f\"Number of desugared results: {len(experimental_results)}\")\n",
    "print(f\"Number of feasible desugared results: {len(list(filter(lambda e: e['feasible'],experimental_results)))}\")\n",
    "print(f\"Number of exact matches: {result_hierarchy[(True, True, True)]}\")\n",
    "print(f\"Number of partial matches: {result_hierarchy[False, True, True]}\")\n",
    "print(f\"Number of unmatched: {sum(v for k, v in result_hierarchy.items() if k not in [(True, True, True), (False, True, True)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for b in baselines:\n",
    "    conf = str(b.get('configuration'))\n",
    "    if conf not in results:\n",
    "        results[conf] = 0\n",
    "    results[conf] += 1\n",
    "print(f\"Number of configurations with warnings: {len(results)}\")\n",
    "\n",
    "for b in baselines:\n",
    "    conf = str(b.get('configuration'))\n",
    "    if results[conf] == 6:\n",
    "        for key in ['id','input_file','input_line','message','warning_path']:\n",
    "            print(key,':',b[key])\n",
    "import random\n",
    "while len(results.keys()) < 1000:\n",
    "    results[''.join(random.choices(string.ascii_letters, k=5))] = 0\n",
    "\n",
    "print(f\"Average warnings per config is {float(sum(results.values()))/float(len(results.values()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hierarchy = {(True, True, True): 0, (False, True, True): 0, (True, False, True): 0, (True, True, False): 0, (False, True, False): 0, (False, False, True): 0, (True, False, False): 0, (False, False, False): 0}\n",
    "finds = 1\n",
    "report = []\n",
    "for e in tqdm.tqdm(experimental_results):\n",
    "    results = [(b, e, match_stats(b, e)) for b in baselines]\n",
    "    found = False\n",
    "    for r in result_hierarchy.keys():\n",
    "        for res in results:\n",
    "            if res[2] == r:\n",
    "                found = True\n",
    "                result_hierarchy[r] += 1\n",
    "                if (r != (True, True, True) and r != (False, True, True)):\n",
    "                    if 'verified' in e.keys() and e['verified'] != 'UNVERIFIED':\n",
    "                        print(finds,e['verified'])\n",
    "                        finds += 1\n",
    "                        for key in ['id','input_file','input_line','message','warning_path','configuration']:\n",
    "                            print(key,':',e[key])\n",
    "                    else:\n",
    "                        for key in ['id','input_file','input_line','message','warning_path']:\n",
    "                            print(key,':',e[key])\n",
    "\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "print('-----------')\n",
    "print(f\"Number of desugared results: {len(experimental_results)}\")\n",
    "print(f\"Number of baseline results: {len(baselines)}\")\n",
    "print(f\"Number of exact matches: {result_hierarchy[(True, True, True)]}\")\n",
    "print(f\"Number of partial matches: {result_hierarchy[False, True, True]}\")\n",
    "print(f\"Number of unmatched: {sum(v for k, v in result_hierarchy.items() if k not in [(True, True, True), (False, True, True)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP\n",
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate & Map FSE Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T00:54:06.010520881Z",
     "start_time": "2023-11-09T00:54:05.315800257Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set Input and Output\n",
    "\n",
    "tool = \"infer\" #clang, infer, phasar\n",
    "program = \"toybox\" #axtls, toybox, busybox\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "from json import JSONEncoder\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "from pathos import multiprocessing\n",
    "import logging\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import re\n",
    "\n",
    "inputLocation = f\"/home/austin/Downloads/SugarlyzerResults/FSEBaselines/baselines/{tool}/{program}/results.json\"\n",
    "mapLocation = f\"/home/austin/Downloads/SugarlyzerResults/FSEMappings/{program}.json\"\n",
    "outputLocation = f\"/home/austin/Downloads/SugarlyzerResults/preparedFSEBaselines/{tool}{program}.json\"\n",
    "\n",
    "baselines: Path = Path(inputLocation)\n",
    "maps: Path = Path(mapLocation)\n",
    "\n",
    "with open(baselines) as f:\n",
    "    baselines = json.load(f)\n",
    "with open(maps) as f:\n",
    "    maps = json.load(f)\n",
    "\n",
    "for b in baselines:\n",
    "    try:\n",
    "        b['original_configuration'] = [re.search(r\"(\\d*)\\.config\", b['input_file']).group(1)]\n",
    "    except AttributeError as ae:\n",
    "        b['original_configuration'] = []\n",
    "    assert(b['original_configuration'] is not None)\n",
    "    parts= b['input_file'].split('/')\n",
    "    b['input_file'] = '/' + '/'.join([parts[1],'/'.join(parts[3:])])\n",
    "    \n",
    "def eq(a,b):\n",
    "    for stat in ['input_file','input_line','message']:\n",
    "        if a[stat] != b[stat]:\n",
    "            return False\n",
    "    if len(a['warning_path']) != len(b['warning_path']):\n",
    "        return False\n",
    "    for w in a['warning_path']:\n",
    "        if w not in b['warning_path']:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "deduped = []\n",
    "for b in baselines:\n",
    "    assert(b['original_configuration'] is not None)\n",
    "    found = False\n",
    "    for d in deduped:\n",
    "        assert(d['original_configuration'] is not None)\n",
    "        if eq(b,d):\n",
    "            found = True\n",
    "            d['original_configuration'].extend(b['original_configuration'])\n",
    "            break\n",
    "    if not found:\n",
    "        deduped.append(b)\n",
    "\n",
    "for d in deduped:\n",
    "    for c in d['configuration']:\n",
    "        fixxer = c[0]\n",
    "        if c[1] == False:\n",
    "            fixxer = \"!DEF_\" + fixxer\n",
    "        elif c[1] == \"y\":\n",
    "            fixxer = \"DEF_\" + fixxer\n",
    "        else:\n",
    "            fixxer = \"USE_\" + fixxer + \" == \" + c[1]\n",
    "        for k,v in maps.items():\n",
    "            if fixxer == v:\n",
    "                temp = k.replace('!DEF','UNDEF')\n",
    "                d['configuration'].append(temp)\n",
    "        \n",
    "with open(outputLocation, 'w') as f:\n",
    "    json.dump(deduped, f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample Configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"/home/austin/Desktop/boxplot_data.csv\", 'w') as outfile:\n",
    "    outfile.write(\"tool,program,iteration,sample_size,num_bugs\")\n",
    "    for tool in ['clang', 'infer', 'phasar']:\n",
    "        for target in ['axtls', 'busybox', 'toybox']:\n",
    "            input_file = f\"/home/austin/Downloads/SugarlyzerResults/preparedFSEBaselines/{tool}{target}.json\"\n",
    "            \n",
    "            with open(input_file) as f:\n",
    "                inputs = json.load(f)\n",
    "                \n",
    "            for j in [10, 50, 100, 500]:\n",
    "                for i in range(50):\n",
    "                    sample = random.sample(range(1000), j)\n",
    "                    bugs = copy.deepcopy(inputs)\n",
    "                    num_bugs = 0\n",
    "                    for s in sample:\n",
    "                        for a in bugs:\n",
    "                            if str(s) in a['original_configuration']:\n",
    "                                num_bugs += 1\n",
    "                                bugs.remove(a)\n",
    "                                \n",
    "                    outfile.write(f\"{tool},{target},{i},{j},{num_bugs}\\n\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T01:04:29.291731628Z",
     "start_time": "2023-11-09T01:01:39.905440126Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deduplicate Varbugs Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Input and Output\n",
    "\n",
    "tool = \"clang\" #clang, infer, phasar\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "from json import JSONEncoder\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "from pathos import multiprocessing\n",
    "import logging\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import re\n",
    "from z3.z3 import Solver, And, Or, Not, Bool, Int, sat\n",
    "\n",
    "inputLocation = f\"VBDbBaselines/{tool}varbugs.json\"\n",
    "outputLocation = f\"preparedVBDbBaselines/{tool}varbugs.json\"\n",
    "\n",
    "baselines: Path = Path(inputLocation)\n",
    "\n",
    "with open(baselines) as f:\n",
    "    baselines = json.load(f)\n",
    "    \n",
    "results = dict()\n",
    "for e in baselines:\n",
    "    key = (e['message'], e['input_file'], e['input_line'])\n",
    "    if key not in results:\n",
    "        results[key] = []\n",
    "    results[key].append(e)\n",
    "\n",
    "def subset(a,b):\n",
    "    al = a['configuration']\n",
    "    bl = b['configuration']\n",
    "    if len(a) == len(b):\n",
    "        return (False,None)\n",
    "    smaller = al if len(al) < len(bl) else bl\n",
    "    bigger = al if len(al) > len(bl) else bl\n",
    "    allIn = True\n",
    "    for x in smaller:\n",
    "        if x not in bigger:\n",
    "            allIn = False\n",
    "            break\n",
    "    if allIn:\n",
    "        aa = copy.deepcopy(a)\n",
    "        aa['configuration'] = smaller\n",
    "        return (True, aa)\n",
    "    return (False, None)\n",
    "\n",
    "def common(a,b):\n",
    "    al = a['configuration']\n",
    "    bl = b['configuration']\n",
    "    if len(a) != len(b):\n",
    "        return (False,None)\n",
    "    swapIn = []\n",
    "    for x in al:\n",
    "        if x not in bl:\n",
    "            swap = x[2:] if x.startswith('UNDEF') else 'UN' + x\n",
    "            if swap not in bl:\n",
    "                return (False, None)\n",
    "        else:\n",
    "            swapIn.append(x)\n",
    "    if len(swapIn) == len(al) -1:\n",
    "        aa = copy.deepcopy(a)\n",
    "        aa['configuration'] = swapIn\n",
    "        return (True, aa)\n",
    "    return (False, None)\n",
    "\n",
    "def eq(a,b):\n",
    "    if len(a) != len(b):\n",
    "        return False\n",
    "    for x in a['configuration']:\n",
    "        if x not in b['configuration']:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "dedupe = []\n",
    "for k,v in results.items():\n",
    "    #v = list of all same bug\n",
    "    anyMatch = True\n",
    "    while anyMatch:\n",
    "        anyMatch = False\n",
    "        newList = []\n",
    "        i = 0\n",
    "        while i < len(v):\n",
    "            matched = False\n",
    "            j = i + 1\n",
    "            while j < len(v):\n",
    "                res = subset(v[i],v[j])\n",
    "                if res[0]:\n",
    "                    newList.append(res[1])\n",
    "                    matched = True\n",
    "                    anyMatch = True\n",
    "                    v.pop(j)\n",
    "                    continue\n",
    "                res = common(v[i],v[j])\n",
    "                if res[0]:\n",
    "                    newList.append(res[1])\n",
    "                    matched = True\n",
    "                    anyMatch = True\n",
    "                    v.pop(j)\n",
    "                    continue\n",
    "                if eq(v[i],v[j]):\n",
    "                    v.pop(j)\n",
    "                    continue\n",
    "                j += 1\n",
    "            if not matched:\n",
    "                newList.append(v[i])\n",
    "            i += 1\n",
    "        v = newList\n",
    "    dedupe.extend(newList)\n",
    "    \n",
    "with open(outputLocation, 'w') as f:\n",
    "    json.dump(dedupe, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate/Clean ExperimentResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Input and Output\n",
    "\n",
    "tool = \"clang\" #clang, infer, phasar\n",
    "program = \"varbugs\" #axtls, toybox, busybox, varbugs, tosemuclibc, tosembusybox, tosemopenssl\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "\n",
    "_default.default = JSONEncoder().default\n",
    "JSONEncoder.default = _default\n",
    "\n",
    "from pathos import multiprocessing\n",
    "import logging\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import re\n",
    "from z3.z3 import Solver, And, Or, Not, Bool, Int, sat\n",
    "\n",
    "inputLocation = f\"experimentResults/{tool}{program}.json\"\n",
    "outputLocation = f\"preparedExperimentResults/{tool}{program}.json\"\n",
    "\n",
    "base: Path = Path(inputLocation)\n",
    "\n",
    "with open(base) as f:\n",
    "    base = json.load(f)\n",
    "\n",
    "def EQ(a,b):\n",
    "    if a['original_line'] != 'ERROR':\n",
    "        if 'verified' in a.keys() and 'verified' in b.keys():\n",
    "            return a['sanitized_message'] == b['sanitized_message'] and a['input_file'] == b['input_file'] and a['original_line'] == b['original_line'] and a['feasible'] == b['feasible'] and a['verified'] == b['verified']\n",
    "        else:\n",
    "            return a['sanitized_message'] == b['sanitized_message'] and a['input_file'] == b['input_file'] and a['original_line'] == b['original_line'] and a['feasible'] == b['feasible']\n",
    "    a1 = a['function_line_range'].split(':')[-1]\n",
    "    a2 = a['function_line_range'].split(':')[-1]\n",
    "    b1 = b['function_line_range'].split(':')[-2]\n",
    "    b2 = b['function_line_range'].split(':')[-1]\n",
    "    return a1 == b1 and a2 == b2 and a['sanitized_message'] == b['sanitized_message'] and a['input_file'] == b['input_file'] and a['feasible'] == b['feasible']\n",
    "\n",
    "print(len(base))\n",
    "i = 0\n",
    "while i < len(base):\n",
    "    if not base[i]['feasible']:\n",
    "        base.pop(i)\n",
    "        continue\n",
    "    j = i+1\n",
    "    while j < len(base):\n",
    "        if EQ(base[i],base[j]):\n",
    "            base[i]['presence_condition'] = 'Or(' + base[i]['presence_condition'] + ',' + base[j]['presence_condition'] +')'\n",
    "            base.pop(j)\n",
    "        j += 1\n",
    "    i += 1\n",
    "print(i)\n",
    "with open(outputLocation,'w') as x:\n",
    "    x.write(json.dumps(base,indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format ToSEM Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = \"busybox\" #uclibc, busybox, openssl\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "\n",
    "inputLocation = f\"tosemBaselines/{program}.json\"\n",
    "outputLocation = f\"preparedTosemBaselines/{program}.json\"\n",
    "\n",
    "base: Path = Path(inputLocation)\n",
    "\n",
    "with open(base) as f:\n",
    "    base = json.load(f)\n",
    "    \n",
    "def getType(msg):\n",
    "    if msg.startswith('Control flow of non-void'): \n",
    "        return 'missingReturn'\n",
    "    elif msg.endswith('freed multiple times!'):\n",
    "        return 'doubleFree'\n",
    "    elif msg.endswith('is used uninitialized!'):\n",
    "        return 'uninitVar'\n",
    "    elif msg.endswith('is freed although not dynamically allocted!'):\n",
    "        return 'freeStatic'\n",
    "    elif msg.endswith('is a dead store!'):\n",
    "        return 'deadStore'\n",
    "    elif msg == 'Case statement is not terminated by a break!':\n",
    "        return 'noBreakSwitch'\n",
    "    elif msg.startswith('incompatible pointer types'):\n",
    "        return 'incompatiblePointers'\n",
    "    elif msg.startswith(\"assignment discards 'const' qualifier\"):\n",
    "        return 'discardQualifier'\n",
    "    elif msg == 'warning: switch statement has dangling code':\n",
    "        return 'danglingSwitch'\n",
    "    print(msg)\n",
    "    err = err # bad break\n",
    "    \n",
    "newRes = []\n",
    "for b in base:\n",
    "    r = {}\n",
    "    f = re.search(r'TypeChef-Sampling-(OpenSSL|Busybox|uClibc)([a-zA-Z0-9\\-\\_/\\.]+\\.c):(\\d+):',b['err'])\n",
    "    if f is None:\n",
    "        if '.h' in b['err']:\n",
    "            continue\n",
    "        print(b['err'])\n",
    "        err = err # bad break\n",
    "    r['input_file'] = f.group(2)\n",
    "    r['input_line'] = int(f.group(3))\n",
    "    f = re.search('W \\[(.*)\\]',b['err'])\n",
    "    r['condition'] = f.group(1)\n",
    "    r['message'] = b['msg']\n",
    "    r['type'] = getType(b['msg'])\n",
    "    newRes.append(r)\n",
    "    \n",
    "with open(outputLocation,'w') as x:\n",
    "    x.write(json.dumps(newRes,indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare ToSEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = \"uclibc\" #uclibc, busybox, openssl\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "def _default(self, obj):\n",
    "    return getattr(obj.__class__, \"to_json\", _default.default)(obj)\n",
    "\n",
    "inputLocation = f\"preparedTosemBaselines/{program}.json\"\n",
    "cexperimentLocation = f\"preparedExperimentResults/clangtosem{program}.json\"\n",
    "pexperimentLocation = f\"preparedExperimentResults/phasartosem{program}.json\"\n",
    "\n",
    "base: Path = Path(inputLocation)\n",
    "clangexpr: Path = Path(cexperimentLocation)\n",
    "phasarexpr: Path = Path(pexperimentLocation)\n",
    "\n",
    "expr = []\n",
    "with open(base) as f:\n",
    "    base = json.load(f)\n",
    "with open(clangexpr) as f:\n",
    "    expr.extend(json.load(f))\n",
    "with open(phasarexpr) as f:\n",
    "    expr.extend(json.load(f))\n",
    "    \n",
    "def sameFile(base, expr):\n",
    "    bf = base['input_file']\n",
    "    ef = expr['input_file']\n",
    "    ef = ef.replace('/targets','').replace('.desugared','')\n",
    "    bf = bf.replace('/study/farce-uclibc','')\n",
    "    return bf == ef\n",
    "def sameLine(base, expr):\n",
    "    bl = base['input_line']\n",
    "    ef = expr['original_line']\n",
    "    if ef == 'ERROR':\n",
    "        return False\n",
    "    start = int(ef.split(':')[0])\n",
    "    end = int(ef.split(':')[1])\n",
    "    return bl >= start and bl <= end\n",
    "def trackExpr(expr,map):\n",
    "    emsg = expr['message']\n",
    "    exprtype = 'other'\n",
    "    if 'Attempt to free released memory' in emsg:\n",
    "        exprtype = 'doubleFree'\n",
    "    elif 'is never read' in emsg:\n",
    "        exprtype = 'deadStore'\n",
    "    elif 'garbage value' in emsg or 'uninitialized value' in emsg or 'Uninitialized Variable' in emsg:\n",
    "        exprtype = 'uninitVar'\n",
    "    map[exprtype] += 1\n",
    " \n",
    "def sameType(base, expr):\n",
    "    emsg = expr['message']\n",
    "    exprtype = ''\n",
    "    if 'Attempt to free released memory' in emsg:\n",
    "        exprtype = 'doubleFree'\n",
    "    elif 'is never read' in emsg:\n",
    "        exprtype = 'deadStore'\n",
    "    elif 'garbage value' in emsg or 'uninitialized value' in emsg or 'Uninitialized Variable' in emsg:\n",
    "        exprtype = 'uninitVar'\n",
    "    return base['type'] == exprtype\n",
    "    \n",
    "\n",
    "def match(base, expr):\n",
    "    return sameFile(base,expr) and sameLine(base,expr) and sameType(base,expr)\n",
    "    \n",
    "canMatch = ['doubleFree','uninitVar','deadStore','discardQualifier','freeStatic','incompatiblePointers']\n",
    "count = 0\n",
    "matches = 0\n",
    "matchTypes = {'doubleFree':0,'uninitVar':0,'deadStore':0,'discardQualifier':0,'freeStatic':0,'incompatiblePointers':0}\n",
    "tosemTypes = {'doubleFree':0,'uninitVar':0,'deadStore':0,'discardQualifier':0,'freeStatic':0,'incompatiblePointers':0, 'other':0}\n",
    "sugarlyzerTypes = {'doubleFree':0,'uninitVar':0,'deadStore':0,'discardQualifier':0,'freeStatic':0,'incompatiblePointers':0, 'other':0}\n",
    "\n",
    "for e in expr:\n",
    "    trackExpr(e,sugarlyzerTypes)\n",
    "\n",
    "for b in base:\n",
    "    if b['type'] in canMatch:\n",
    "        count += 1\n",
    "        tosemTypes[b['type']] += 1\n",
    "    else:\n",
    "        tosemTypes['other'] += 1\n",
    "    for e in expr:\n",
    "        if match(b,e):\n",
    "            matches += 1\n",
    "            matchTypes[b['type']] += 1 \n",
    "            break\n",
    "\n",
    "print('viable alarms',count, 'matched alarms', matches)\n",
    "print (matchTypes)\n",
    "print(tosemTypes)\n",
    "print(sugarlyzerTypes)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "infeasible = 0\n",
    "\n",
    "for tool in ['clang','infer','phasar']:\n",
    "    for prog in ['axtls','toybox','busybox','varbugs','tosemuclibc','tosemopenssl','tosembusybox']:\n",
    "        with open(f'experimentResults/{tool}{prog}.json') as i:\n",
    "            alarms = json.load(i)\n",
    "        for a in alarms:\n",
    "            if not a['feasible']:\n",
    "                infeasible += 1\n",
    "print(infeasible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
